<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hand-Track Study</title>
</head>

<body>
    <video id="videoInput"></video>
    <canvas id="canvasFrame"></canvas>
    <canvas id="binary"></canvas>

    <p id="info"></p>

    <script async src="libs/opencv0.js" onload="start({cv: true});" type="text/javascript"></script>
    <script>
        // variables to know when both opencv and camera loaded
        let cvHasLoaded = false,
            cameraHasLoaded = false;

        // canvas context and video
        const context = document.getElementById("canvasFrame").getContext("2d");
        const video = document.getElementById("videoInput");

        // start video and get its properties
        let width, height;

        startVideo(video)
            .then((value) => {
                width = value.width;
                height = value.height;
                start({ camera: true })
            });


        function start(info) {
            cvHasLoaded = cvHasLoaded || info.cv;
            cameraHasLoaded = cameraHasLoaded || info.camera;

            console.log(cvHasLoaded, cameraHasLoaded);

            const p = document.getElementById("info");
            p.innerHTML += `cv: ${cvHasLoaded}, cam: ${cameraHasLoaded} |`;

            if (cvHasLoaded && cameraHasLoaded) {
                main();
            }
        }

        function main() {
            // -- Constants -- //
            const FPS = 24;
            const BLACK = new cv.Scalar(0, 0, 0, 255); // helper scalar for black color
            const RED = new cv.Scalar(255, 0, 0); // helper scalar for red color

            // Thesholds for pixel color classification
            const LOW = new cv.Mat(height, width, cv.CV_8UC3, [0, 133, 77, 0]);
            const HIGH = new cv.Mat(height, width, cv.CV_8UC3, [255, 173, 127, 255]);

            // Minimum occupied area threshold
            const MIN_AREA = height * width * 0.1;

            // Minimum number of feature points
            const MIN_FEATURES = 20;

            // --------------- //

            // Video manipulation variables
            const src = new cv.Mat(height, width, cv.CV_8UC4); // storages image source
            const dst = new cv.Mat(height, width, cv.CV_8UC4); // storages final result
            const binaryMask = new cv.Mat(height, width, 0);
            const aux = new cv.Mat(height, width, cv.CV_8UC4); // helper mat - rename it
            const grayFrame = new cv.Mat(height, width, cv.CV_8UC4);
            const oldFrame = new cv.Mat(height, width, cv.CV_8UC4); // helper mat - rename it
            const mask = new cv.Mat(height, width, cv.CV_8UC4, BLACK); // helper mat

            // Contourns variables - https://docs.opencv.org/3.4.15/d5/daa/tutorial_js_contours_begin.html
            const contours = new cv.MatVector(); //storages contours
            const hierarchy = new cv.Mat(); // stores hierarchy
            const contourArea = { id: -1, value: 0 } // stores the larger contourn and its id

            // features variables - https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541
            let [maxCorners, qualityLevel, minDistance, blockSize] = [40, 0.2, 15, 7];
            let features = new cv.Mat();
            let hasFeatures = false;

            // optical flow variables - https://docs.opencv.org/3.4/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323
            let features2 = new cv.Mat(); // -rename it
            let winSize = new cv.Size(15, 15);
            let maxLevel = 2;
            let criteria = new cv.TermCriteria(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03);
            let stats = new cv.Mat();
            let err = new cv.Mat()
            let goodNew = [];
            let goodOld = [];

            // hull variables -
            let tempHull = new cv.MatVector();
            const fingerTips = [];

            // colors for drawing points
            const colors = [];
            for (let i = 0; i < maxCorners; i++) {
                colors.push(new cv.Scalar(parseInt(Math.random() * 255), parseInt(Math.random() * 255), parseInt(Math.random() * 255), 255));
            }

            let begin, delay; // fps helpers
            function processVideo() {
                begin = Date.now();

                context.drawImage(video, 0, 0, width, height);
                src.data.set(context.getImageData(0, 0, width, height).data);

                classifyPixesl(src, aux, LOW, HIGH);

                findBiggestArea(aux, contours, contourArea)

                if (contourArea.value < MIN_AREA) {
                    hasFeatures = false;
                    dst.setTo(BLACK);
                }
                else {
                    dst.setTo(BLACK)
                    detachForeground(src, dst, contours, contourArea.id);

                    // dst.copyTo(grayFrame);
                    cv.cvtColor(dst, grayFrame, cv.COLOR_RGB2GRAY);

                    if (!hasFeatures) {
                        cv.goodFeaturesToTrack(grayFrame, features, maxCorners, qualityLevel, minDistance);

                        if (features.rows >= MIN_FEATURES) {
                            hasFeatures = true;
                            mask.setTo(BLACK);
                            grayFrame.copyTo(oldFrame);
                        }
                    }
                    else {
                        cv.calcOpticalFlowPyrLK(oldFrame, grayFrame, features, features2, stats, err, winSize, maxLevel, criteria);

                        goodNew.length = 0;
                        goodOld.length = 0;

                        for (let i = 0; i < stats.rows; i++) {
                            if (stats.data[i] === 1) {
                                goodNew.push(new cv.Point(features2.data32F[i * 2], features2.data32F[i * 2 + 1]));
                                goodOld.push(new cv.Point(features.data32F[i * 2], features.data32F[i * 2 + 1]));
                            }
                        }

                        // for (let i = 0; i < goodNew.length; i++) {
                        //     cv.circle(dst, goodNew[i], 5, colors[i], -1);
                        //     // cv.line(mask, goodNew[i], goodOld[i], colors[i], 2);
                        // }

                        obtainFingers(contours.get(contourArea.id), fingerTips);


                        for (let i = 0; i < fingerTips.length; i++) {
                            cv.circle(dst, fingerTips[i], 5, colors[i], 1);
                            // cv.line(mask, goodNew[i], goodOld[i], colors[i], 2);
                        }
                        // return;

                        // mask.setTo(BLACK);
                        // cv.drawContours(mask, tempHull, 0, colors[0], 2, 8, hierarchy, 0);
                        cv.add(dst, mask, dst);

                        grayFrame.copyTo(oldFrame);
                        for (let i = 0; i < goodNew.length; i++) {
                            features.data32F[i * 2] = goodNew[i].x;
                            features.data32F[i * 2 + 1] = goodNew[i].y;
                        }
                    }
                }

                cv.imshow("canvasFrame", dst);


                delay = 1000 / FPS - (Date.now() - begin);
                setTimeout(processVideo, delay);
            }

            setTimeout(processVideo, 0);

            function classifyPixesl(source, destination,) {
                cv.cvtColor(source, destination, cv.COLOR_RGB2YCrCb);
                cv.inRange(destination, LOW, HIGH, destination);
            }

            function findBiggestArea(source, contours, contourArea) {
                cv.findContours(source, contours, hierarchy, cv.RETR_CCOMP, cv.CHAIN_APPROX_SIMPLE);

                let areaIdx = -1, area, biggerArea = 0;

                for (i = 0; i < contours.size(); i++) {
                    contour = contours.get(i);
                    area = cv.contourArea(contour, false);

                    if (area > biggerArea) {
                        areaIdx = i;
                        biggerArea = area;
                    }
                }

                contourArea.id = areaIdx;
                contourArea.value = biggerArea;
            }

            function obtainFingers(contour, fingerTips) {
                tempHull.delete();
                tempHull = new cv.MatVector();

                let tmp = new cv.Mat();

                cv.convexHull(contour, tmp, false, false);
                tempHull.push_back(tmp);

                tmp.data32S.sort((a, b) => a - b);

                // define cluster as being a group of hull indexes that are less than 100 units apart
                let firstMember = tmp.data32S[0]; // indicates the first member of a cluster
                let firstIndex = 0; // member index in the array data32S
                let counter = 0;

                fingerTips.length = 0;
                for (let i = 0; i < tmp.rows; i++) {
                    let member = tmp.data32S[i];

                    if (Math.abs(firstMember - member) > 100) {
                        let middleIndex = firstIndex + Math.trunc(counter / 2);
                        let middleMember = tmp.data32S[middleIndex];

                        fingerTips.push(new cv.Point(
                            contour.data32S[middleMember * 2],
                            contour.data32S[middleMember * 2 + 1]
                        ));

                        firstIndex = i;
                        firstMember = member;
                        counter = 0;
                    }
                    counter++;
                }

                tmp.delete();
            }

            function detachForeground(source, destination, contours, areaIdx) {
                binaryMask.setTo(BLACK);

                cv.drawContours(binaryMask, contours, areaIdx, RED, -1, cv.LINE_8, hierarchy, 1);
                cv.bitwise_and(source, source, destination, binaryMask);
            }
        }


        async function startVideo(video) {
            let stream = null; // video stream
            let width, height; // video size

            const constraints = {
                video: {
                    facingMode: "environment",
                    width: 480,
                    height: 360
                },
                audio: false
            }

            try {
                stream = await navigator.mediaDevices.getUserMedia(constraints);
                video.srcObject = stream;
                video.play();

                width = stream.getVideoTracks()[0].getSettings().width
                height = stream.getVideoTracks()[0].getSettings().height

                return { width, height }
            }
            catch (err) {
                console.error('Error accesing camera:', err);
                const p = document.getElementById("info");
                p.innerHTML = 'Error accesing camera - ' + err;
            }
        }
    </script>
</body>

</html>